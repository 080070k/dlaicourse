{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ka96-ajYzxVU"
   },
   "source": [
    "# Train Your Own Model and Convert It to TFLite\n",
    "\n",
    "This notebook uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing we'll use here.\n",
    "\n",
    "This uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
    "\n",
    "We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjOAfhgd__Sp"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pfyZKowNAQ4j",
    "outputId": "8a94ac17-d4e7-474f-e984-a5ed389f5352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Using TensorFlow Version: 2.0.0\n",
      "• GPU Device Not Found. Running on CPU\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow Datsets\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "# Helper Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "\n",
    "print('\\u2022 Using TensorFlow Version:', tf.__version__)\n",
    "print('\\u2022 GPU Device Found.' if tf.test.is_gpu_available() else '\\u2022 GPU Device Not Found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tadPBTEiAprt"
   },
   "source": [
    "# Download Fashion MNIST Dataset\n",
    "\n",
    "We will use TensorFlow Datasets to load the Fashion MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "XcNwi6nFKneZ",
    "outputId": "8e0d8173-6dbd-4ef5-a70b-efc8e9d33802"
   },
   "outputs": [],
   "source": [
    "splits = tfds.Split.ALL.subsplit(weighted=(80, 10, 10))\n",
    "\n",
    "splits, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True, split=splits)\n",
    "\n",
    "(train_examples, validation_examples, test_examples) = splits\n",
    "\n",
    "num_examples = info.splits['train'].num_examples\n",
    "num_classes = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class names are not included with the dataset, so we will specify them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-eAv71FRm4JE"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXe6jNokqX3_"
   },
   "outputs": [],
   "source": [
    "# Create a labels.txt file with the class names\n",
    "with open('labels.txt', 'w') as f:\n",
    "    f.write('\\n'.join(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iubWCThbdN8K"
   },
   "outputs": [],
   "source": [
    "# The images in the dataset are 28 by 28 pixels.\n",
    "IMG_SIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAkuq0V0Aw2X"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5SIivkunKCC"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwyhsyGydHDl"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Write a function to normalize the images.\n",
    "\n",
    "def format_example(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAlBlXOUMwqe"
   },
   "outputs": [],
   "source": [
    "# Specify the batch size\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JM4HfIJtnNEk"
   },
   "source": [
    "## Create Datasets From Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxe2I3oxLDhq"
   },
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "train_batches = train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
    "validation_batches = validation_examples.cache().batch(BATCH_SIZE).map(format_example)\n",
    "test_batches = test_examples.map(format_example).batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-topQaOm_LM"
   },
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
    "_________________________________________________________________\n",
    "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 3872)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 64)                247872    \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                650       \n",
    "=================================================================\n",
    "Total params: 253,322\n",
    "Trainable params: 253,322\n",
    "Non-trainable params: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDqcwksFB1bh"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Build and compile the model shown in the previous cell.\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEMOz-LDnxgD"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGlNoRtzCP4_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 14s 65ms/step - loss: 0.5971 - accuracy: 0.7933 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 8s 38ms/step - loss: 0.3642 - accuracy: 0.8731 - val_loss: 0.3222 - val_accuracy: 0.8856\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.3219 - accuracy: 0.8860 - val_loss: 0.2971 - val_accuracy: 0.8974\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.2936 - accuracy: 0.8960 - val_loss: 0.2824 - val_accuracy: 0.8981\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.2684 - accuracy: 0.9041 - val_loss: 0.2563 - val_accuracy: 0.9090\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 8s 38ms/step - loss: 0.2475 - accuracy: 0.9112 - val_loss: 0.2554 - val_accuracy: 0.9073\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 8s 38ms/step - loss: 0.2344 - accuracy: 0.9155 - val_loss: 0.2372 - val_accuracy: 0.9140\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 8s 38ms/step - loss: 0.2216 - accuracy: 0.9181 - val_loss: 0.2281 - val_accuracy: 0.9169\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 8s 38ms/step - loss: 0.2083 - accuracy: 0.9239 - val_loss: 0.2284 - val_accuracy: 0.9164\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 0.1965 - accuracy: 0.9281 - val_loss: 0.2265 - val_accuracy: 0.9199\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_batches, epochs=10, validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZT9-7w9n4YO"
   },
   "source": [
    "# Exporting to TFLite\n",
    "\n",
    "You will now save the model to TFLite. We should note, that you will probably see some warning messages when running the code below. These warnings have to do with software updates and should not cause any errors or prevent your code from running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "9dq78KBkCV2_",
    "outputId": "41b1e939-7c48-4859-a552-d6961ec38d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Use the tf.saved_model API to save your model in the SavedModel format. \n",
    "export_dir = 'saved_model/1'\n",
    "\n",
    "tf.saved_model.save(model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "EDGiYrBdE6fl"
   },
   "outputs": [],
   "source": [
    "# Select mode of optimization\n",
    "mode = \"Speed\" \n",
    "\n",
    "if mode == 'Storage':\n",
    "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
    "elif mode == 'Speed':\n",
    "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
    "else:\n",
    "    optimization = tf.lite.Optimize.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbcS9C00CzGe"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Use the TFLiteConverter SavedModel API to initialize the converter\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "converter.optimizations = [optimization]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q5PWCDsTC3El",
    "outputId": "97349e68-0bff-41cd-ad48-90a6abb85f11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258656"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file = pathlib.Path('./model.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SR6wFcQ1Fglm"
   },
   "source": [
    "# Test the Model with TFLite Interpreter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKcToCBEC-Bu"
   },
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8EpFpIBFkq8"
   },
   "outputs": [],
   "source": [
    "# Gather results for the randomly sampled test images\n",
    "predictions = []\n",
    "test_labels = []\n",
    "test_images = []\n",
    "\n",
    "for img, label in test_batches.take(50):\n",
    "    interpreter.set_tensor(input_index, img)\n",
    "    interpreter.invoke()\n",
    "    predictions.append(interpreter.get_tensor(output_index))\n",
    "    test_labels.append(label[0])\n",
    "    test_images.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "kSjTmi05Tyod"
   },
   "outputs": [],
   "source": [
    "# Utilities functions for plotting\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    img = np.squeeze(img)\n",
    "    \n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    \n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    \n",
    "    if predicted_label == true_label.numpy():\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        \n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                         100*np.max(predictions_array),\n",
    "                                         class_names[true_label]),\n",
    "                                         color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(list(range(10)))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array[0], color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array[0])\n",
    "    \n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "ZZwg0wFaVXhZ",
    "outputId": "f9676edc-f305-4115-938b-389286d2228d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the outputs\n",
    "\n",
    "# Select the index of the image to display. Minimum index value is 1 and max index value is 50. \n",
    "index = 49\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(index, predictions, test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(index, predictions, test_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8t7_jRiz9Vw"
   },
   "source": [
    "# Prepare the Test Images for Download (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi09nIps0gBu"
   },
   "outputs": [],
   "source": [
    "!mkdir -p test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sF7EZ63J0hZs"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for index, (image, label) in enumerate(test_batches.take(50)):\n",
    "    image = tf.cast(image * 255.0, tf.uint8)\n",
    "    image = tf.squeeze(image).numpy()\n",
    "    pil_image = Image.fromarray(image)\n",
    "    pil_image.save('test_images/{}_{}.jpg'.format(class_names[label[0]].lower(), index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uM35O-uv0iWS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ankle boot_13.jpg  coat_42.jpg        sandal_17.jpg      sneaker_22.jpg\r\n",
      "ankle boot_16.jpg  coat_8.jpg         sandal_20.jpg      sneaker_31.jpg\r\n",
      "ankle boot_18.jpg  dress_1.jpg        sandal_28.jpg      sneaker_37.jpg\r\n",
      "ankle boot_49.jpg  dress_11.jpg       sandal_32.jpg      sneaker_40.jpg\r\n",
      "bag_15.jpg         dress_12.jpg       sandal_47.jpg      sneaker_44.jpg\r\n",
      "bag_24.jpg         dress_21.jpg       shirt_3.jpg        t-shirt_top_41.jpg\r\n",
      "bag_25.jpg         dress_45.jpg       shirt_33.jpg       t-shirt_top_43.jpg\r\n",
      "bag_29.jpg         dress_46.jpg       shirt_38.jpg       trouser_0.jpg\r\n",
      "bag_34.jpg         pullover_23.jpg    shirt_4.jpg        trouser_14.jpg\r\n",
      "bag_5.jpg          pullover_26.jpg    shirt_6.jpg        trouser_2.jpg\r\n",
      "bag_7.jpg          pullover_36.jpg    shirt_9.jpg        trouser_30.jpg\r\n",
      "coat_27.jpg        pullover_39.jpg    sneaker_10.jpg\r\n",
      "coat_35.jpg        pullover_48.jpg    sneaker_19.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!ls test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aR20r4qW0jVm"
   },
   "outputs": [],
   "source": [
    "!tar --create --file=fmnist_test_images.tar test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_Lite_Week_1_Exercise.ipynb        model.tflite\r\n",
      "TF_Lite_Week_1_Exercise_Answer.ipynb \u001b[34msaved_model\u001b[m\u001b[m\r\n",
      "fmnist_test_images.tar               \u001b[34mtest_images\u001b[m\u001b[m\r\n",
      "labels.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF Lite Week 1 Exercise - Answer",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
